{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "% matplotlib inline\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.linalg as alg\n",
    "import scipy as spy\n",
    "import networkx as nx\n",
    "\n",
    "import time\n",
    "from itertools import *\n",
    "import sys\n",
    "import numpy.linalg as LA\n",
    "import pickle\n",
    "# set hyperparameter Lambda and Rho\n",
    "Lambda = 0.1\n",
    "Rho = 1\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import scipy as spy\n",
    "import time\n",
    "from itertools import *\n",
    "import sys\n",
    "import cvxpy as cvx\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy import sparse as sp\n",
    "import networkx as nx\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from scipy.special import expit\n",
    "from sklearn import linear_model, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ADMM:\n",
    "    '''\n",
    "    ADMM for graph regularization Python class\n",
    "    input: \n",
    "        X: feature matrix, N*d matrix\n",
    "        y: N*1 label vector, where y_i = 0, if node i is in test indices\n",
    "        G: graph with N nodes as a nested dictionary\n",
    "        Lambda: hyperparameter to control graph regularization\n",
    "        Rho: hyperparameter to control ADMM stepsize\n",
    "        train_mask: N*1 boolean vector\n",
    "        test_mask: N*1 boolean vector\n",
    "        y_true: N*1 label vector\n",
    "        Threshold:hyperparameter as the stopping criteria of ADMM algorithm\n",
    "        paramters, features, labels, and graph structure\n",
    "    output: \n",
    "        W: estimated W, d*1 vector\n",
    "        b: estimated b, N*1 vector\n",
    "        losses: losses per iteration\n",
    "    '''\n",
    "    def __init__(self, X, y, G, nodes, edgeNbr, Lambda, Rho, train_mask, test_mask, y_true, Threshold, initialW, initialb):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.Threshold = Threshold\n",
    "        self.y_true = y_true\n",
    "        self.train_mask = train_mask\n",
    "        self.test_mask = test_mask\n",
    "        self.dim = X.shape[1]\n",
    "        self.Lambda = Lambda\n",
    "        self.Rho = Rho\n",
    "        self.graph = G\n",
    "        self.nodes = nodes\n",
    "\n",
    "        row=[]\n",
    "        col=[]\n",
    "        for i, js in self.graph.items():\n",
    "            for j in js:\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "                \n",
    "        initialZ=np.random.rand(len(row))\n",
    "        initialU=np.random.rand(len(row))\n",
    "        self.Z = collections.defaultdict(dict)\n",
    "        self.U = collections.defaultdict(dict)\n",
    "        k=0\n",
    "        for i, js in self.graph.items():\n",
    "            for j in js:\n",
    "                self.Z[i][j]=initialZ[k]\n",
    "                self.U[i][j]=initialU[k]\n",
    "                k+=1\n",
    "\n",
    "        # set the initial value of W and b with the logistics regression result\n",
    "        self.W = initialW.reshape(X.shape[1])\n",
    "        self.b = np.ones(X.shape[0])*initialb\n",
    "\n",
    "    def dumpWb(self, filename):\n",
    "        dict = {\"W\": self.W, \"b\": self.b}\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump( dict, f)\n",
    "        \n",
    "    def deriv_b(self, b, C1, C2, C3, eC1):\n",
    "        if (eC1 == float('inf')):\n",
    "            return C2 + C3 *b\n",
    "    \n",
    "        return 1/(1+ eC1* math.exp(-1.0*b)) + C2 + C3*b\n",
    "\n",
    "    def deriv_b_negy(self, b, C1, C2, C3, eC1):\n",
    "        if (eC1 == float('inf')):\n",
    "            return C2 + C3 *b\n",
    "        \n",
    "        return -1.0/(1+ eC1* math.exp(b)) + C2 + C3*b\n",
    "    \n",
    "    def update_b(self):\n",
    "        '''\n",
    "        update the value of b, check line 4 of the ADMM algorithm for the math\n",
    "        cvxpy is conducted independently for each node\n",
    "        '''\n",
    "        B = []\n",
    "        num_nodes = len(self.nodes)\n",
    "        kk=0\n",
    "        for i in self.nodes:\n",
    "            #if kk%1000000==0:\n",
    "            #    self.logger.info('update_b: {0} %{1:4.2f}'.format(i, kk *1.0 /num_nodes *100 ))\n",
    "            #    kk+=1\n",
    "                \n",
    "            sumdiffZU = 0\n",
    "            neighborCnt = 0\n",
    "            for Id in self.graph[i]:\n",
    "                sumdiffZU += (self.Z[i][Id]-self.U[i][Id])\n",
    "                neighborCnt += 1\n",
    "\n",
    "            if (neighborCnt == 0):\n",
    "                 raise ValueError('{0} has no neighbor'.format(i))\n",
    "\n",
    "            b1 = sumdiffZU /neighborCnt\n",
    "\n",
    "            #in case of missing value, we have analytical solution for b\n",
    "            if (self.y[i]==0):\n",
    "                self.b[i]= b1\n",
    "                continue\n",
    "\n",
    "            tol = 1e-5\n",
    "\n",
    "            #the optimial value is within the interval [b1, b2]\n",
    "            if (self.y[i]==1):\n",
    "                b2 = b1 + 1/self.Rho/neighborCnt\n",
    "                #bisection method to find a better b\n",
    "                C1 = -1.0 * self.X[i].dot(self.W) #C1 = -1.0 * self.X[i].dot(self.g[i,:])\n",
    "                C2 = -1-self.Rho * sumdiffZU\n",
    "                C3 = self.Rho * neighborCnt\n",
    "                eC1 = 0\n",
    "                try:\n",
    "                    eC1 = math.exp(C1)\n",
    "                except OverflowError:\n",
    "                    eC1 = float('inf')\n",
    "                while(b2-b1 > tol):\n",
    "                    Db1 = self.deriv_b(b1, C1, C2, C3, eC1)\n",
    "                    Db2 = self.deriv_b(b2, C1, C2, C3, eC1)\n",
    "                    if (math.fabs(Db1)<tol):\n",
    "                        b2 = b1\n",
    "                        break;\n",
    "\n",
    "                    if (math.fabs(Db2)<tol):\n",
    "                        b1 = b2\n",
    "                        break;\n",
    "                    \n",
    "                    if (not(Db1<=tol and Db2>=-1.0*tol)):\n",
    "                        raise ValueError('Db1 and Db2 has same sign which is impossible! Db1={0}, Db2={1}, b1={2}, b2={3}'.format(Db1, Db2, b1, b2))\n",
    "                    \n",
    "                    b3 = (b1 + b2)/2\n",
    "                    Db3 = self.deriv_b(b3, C1, C2, C3, eC1)\n",
    "                    if (Db3 >=0):\n",
    "                        b2=b3\n",
    "                    else:\n",
    "                        b1=b3\n",
    "                    \n",
    "                self.b[i] = (b1 + b2)/2\n",
    "                continue\n",
    "\n",
    "            if (self.y[i]==-1):\n",
    "                b2 = b1\n",
    "                b1 = b2 - 1/self.Rho/neighborCnt\n",
    "                C1 = self.X[i].dot(self.W) #C1 = self.X[i].dot(self.g[i,:])\n",
    "                C2 = 1-self.Rho * sumdiffZU\n",
    "                C3 = self.Rho * neighborCnt\n",
    "                eC1 = 0\n",
    "                try:\n",
    "                    eC1 = math.exp(C1)\n",
    "                except OverflowError:\n",
    "                    eC1 = float('inf')\n",
    "                    \n",
    "                while(b2-b1 > tol):\n",
    "                    Db1 = self.deriv_b_negy(b1, C1, C2, C3, eC1)\n",
    "                    Db2 = self.deriv_b_negy(b2, C1, C2, C3, eC1)\n",
    "                    if (math.fabs(Db1)<tol):\n",
    "                        b2 = b1\n",
    "                        break;\n",
    "\n",
    "                    if (math.fabs(Db2)<tol):\n",
    "                        b1 = b2\n",
    "                        break;\n",
    "                    \n",
    "                    if (not(Db1<=tol and Db2>=-1.0*tol)):\n",
    "                        raise ValueError('Db1 and Db2 has same sign which is impossible! Db1={0}, Db2={1}, b1={2}, b2={3}, C1={4}, C2={5}, C3={6}'.format(\n",
    "                            Db1, Db2, b1, b2, C1, C2, C3))\n",
    "                    \n",
    "                    b3 = (b1 + b2)/2\n",
    "                    Db3 = self.deriv_b_negy(b3, C1, C2, C3, eC1)\n",
    "                    if (Db3 >=0):\n",
    "                        b2=b3\n",
    "                    else:\n",
    "                        b1=b3\n",
    "                        \n",
    "                    \n",
    "                self.b[i] = (b1 + b2)/2\n",
    "                continue\n",
    "            \n",
    "            raise ValueError('impossible value for y={0}'.format(self.y[i]))\n",
    "        \n",
    "    def update_Z(self):\n",
    "        '''\n",
    "        update the value of Z, check line 6 of the ADMM algorithm for the math\n",
    "        rho is lambda times rho2\n",
    "        f is L_{rho}(W_t^{k+1}, b_t^{k+1}, g^{k+1}, (z_{ij}, z_{ji}, z_{(ij)^c}^k, u^k, h^k; t)\n",
    "        see page 5 of https://arxiv.org/pdf/1703.07520.pdf Social discrete choice model\n",
    "        '''\n",
    "        for k in self.graph:\n",
    "            for j in self.graph[k]:\n",
    "                A = self.b[j] + self.U[j][k]\n",
    "                B = self.b[k] + self.U[k][j]\n",
    "                self.Z[k][j] = (2*self.Lambda*A + (2*self.Lambda+self.Rho)*B)/(self.Lambda*4+self.Rho)\n",
    "\n",
    "    def update_U(self):\n",
    "        '''\n",
    "        update the value of U, check line 7 of the ADMM algorithm for the math\n",
    "        '''\n",
    "        for i in self.graph:\n",
    "            for Id in self.graph[i]:\n",
    "                self.U[i][Id] = self.U[i][Id] + self.b[i] - self.Z[i][Id]\n",
    "\n",
    "                \n",
    "    '''\n",
    "    using a simple gradient descent algorithm to update W\n",
    "    https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf\n",
    "    learning rate is chosen using Backtracking linear search. see page 10 of the slides above\n",
    "    '''\n",
    "    def update_W(self, iteration):\n",
    "        featureCnt = len(self.W)\n",
    "        maxiter = 2\n",
    "        oldloss = self.cal_LL()\n",
    "        newloss = oldloss\n",
    "        for k in range(maxiter):\n",
    "            learningrate = 0.00001\n",
    "            oldloss = newloss\n",
    "            print('update W iteration {0}.{1}'.format(iteration, k))\n",
    "            gradient = np.zeros(featureCnt)\n",
    "            for i in self.graph:\n",
    "                if (self.y[i]==0):\n",
    "                    continue\n",
    "                C1 = -1.0 * self.y[i]* (self.X[i].dot(self.W) + self.b[i])\n",
    "                eC1 = 0\n",
    "                multiplier = 0\n",
    "                try:\n",
    "                    eC1 = math.exp(C1)\n",
    "                except OverflowError:\n",
    "                    eC1 = float('inf')\n",
    "                if (eC1==float('inf')):\n",
    "                    multiplier =  -1.0*self.y[i]\n",
    "                else:\n",
    "                    multiplier = (1 - 1.0/(1.0+eC1)) * (-1.0)*self.y[i]\n",
    "                    \n",
    "                gradient = np.add(gradient, multiplier * self.X[i])\n",
    "\n",
    "            gradientNorm = np.linalg.norm(gradient)\n",
    "\n",
    "            if (gradientNorm == float('inf')):\n",
    "                raise ValueError('norm of gradient is infinity') #should never happen\n",
    "\n",
    "            gradientNorm2 = gradientNorm * gradientNorm\n",
    "            \n",
    "            oldW = np.copy(self.W)\n",
    "            kk=0\n",
    "            newloss = 0\n",
    "            tol = 1e-5\n",
    "            while (True):\n",
    "                np.copyto(self.W, oldW)\n",
    "                self.W -= learningrate * gradient\n",
    "                anticipateddecrease = learningrate * gradientNorm2 /2.0\n",
    "                print('anticipate the loss to decrease from {0} by {1}'.format(oldloss, anticipateddecrease))\n",
    "            \n",
    "                targetloss = oldloss - anticipateddecrease\n",
    "                \n",
    "                try:\n",
    "                    newloss = self.cal_LL()\n",
    "                except OverflowError:\n",
    "                    learningrate = learningrate / 2\n",
    "                    kk+=1\n",
    "                    print('get infinite loss, reduce learning rate to {0}, kk={1}'.format(learningrate, kk))\n",
    "                    continue\n",
    "                \n",
    "                if (newloss <= targetloss + tol):\n",
    "                    break;\n",
    "                \n",
    "                learningrate  = learningrate / 2\n",
    "                \n",
    "                kk+=1\n",
    "                print('loss is not decreasing below anticipated value, reduce learning rate to {0}, kk={1}'.format(learningrate, kk))\n",
    "                if(kk>1000):\n",
    "                    raise ValueError('cannot find a good learning rate to get finite loss.learningrate={0}'.format(learningrate))\n",
    "                \n",
    "            print('learning rate: {0}'.format(learningrate))\n",
    "            print('max in gradient is :{0}'.format(np.max(np.abs(gradient))))\n",
    "            print('oldloss:' + str(oldloss) + ',newloss:' + str(newloss))\n",
    "            if(math.fabs(newloss-oldloss) < 0.00001 * oldloss):\n",
    "                return newloss\n",
    "        return newloss\n",
    "            \n",
    "    def optimize_b(self, iterations, old_loss, verbose=False):\n",
    "        kk = 0\n",
    "        maxiter = 5\n",
    "        while (True):\n",
    "            start2 = time.time()\n",
    "            self.update_b()\n",
    "            end2 = time.time()\n",
    "            if(verbose):\n",
    "                print('finished b {0} seconds at iteration {1}'.format(end2-start2, iterations))\n",
    "            start2 = time.time()\n",
    "            self.update_Z()\n",
    "            end2 = time.time()\n",
    "            if(verbose):\n",
    "                print('finished Z {0} seconds at iteration {1}'.format(end2-start2, iterations))\n",
    "            start2 = time.time()\n",
    "            self.update_U()\n",
    "            end2 = time.time()\n",
    "            if(verbose):\n",
    "                print('finished U {0} seconds at iteration {1}'.format(end2-start2, iterations))\n",
    "            loss = self.cal_LL()\n",
    "            print('loss is {0}, old loss is {1} at iteration {2}.{3}'.format(loss, old_loss, iterations, kk))\n",
    "            kk+=1\n",
    "            if(np.absolute(old_loss-loss)<=self.Threshold):\n",
    "                return loss\n",
    "            if (kk > maxiter):\n",
    "                return loss\n",
    "                        \n",
    "    def runADMM_Grid(self):\n",
    "        '''\n",
    "        runADMM Grid iterations\n",
    "        The stopping criteria is when the difference of the value of the objective function in current iteration\n",
    "        and the value of the objective function in the previous iteration is smaller than the Threshold\n",
    "        '''\n",
    "        resultdump = 'result.dump'\n",
    "#         self.dumpWb(resultdump + \".initial\")\n",
    "        self.losses = []\n",
    "        self.times = []\n",
    "        loss = self.cal_LL()\n",
    "        self.losses.append(loss)\n",
    "        print('iteration = 0')\n",
    "        print('objective = {0}'.format(loss))\n",
    "        old_loss = loss\n",
    "        loss = float('inf')\n",
    "        iterations = 0\n",
    "        import time\n",
    "        start = time.time()\n",
    "        while(True):\n",
    "            loss = self.optimize_b(iterations, old_loss)\n",
    "            \n",
    "            start2 = time.time()\n",
    "            loss = self.update_W(iterations)\n",
    "            end2 = time.time()\n",
    "            print('finished w {0} seconds at iteration {1}'.format(end2-start2, iterations))\n",
    "            print('loss is {0}, old loss is {1} at iteration {2}'.format(loss, old_loss, iterations))\n",
    "            loss = self.cal_LL()\n",
    "            self.losses.append(loss)\n",
    "            if(np.absolute(old_loss- loss) <=  self.Threshold):\n",
    "                break\n",
    "            old_loss = loss\n",
    "            iterations += 1\n",
    "#             if (iterations % 2 == 0):\n",
    "#                 self.dumpWb(resultdump + \".\" + str(iterations))\n",
    "\n",
    "        print('total iterations = ' + str(iterations))\n",
    "        end = time.time()\n",
    "        print('total time = {0}'.format(end-start))\n",
    "#         self.dumpWb(resultdump + \".final\" )\n",
    "\n",
    "    def cal_LL(self):\n",
    "        '''\n",
    "        function to calculate the value of loss function\n",
    "        '''\n",
    "        W = np.array(self.W).flatten()\n",
    "        b = np.array(self.b).flatten()\n",
    "        loss = 0\n",
    "        for i in self.nodes:\n",
    "            r = np.log(1 + np.exp(-self.y[i]*(np.dot(self.X[i], W) + b[i])))\n",
    "            if(r == float('inf')):\n",
    "                raise OverflowError('loss is infinity')\n",
    "            loss += r\n",
    "        \n",
    "        for i, js in self.graph.items():\n",
    "            for j in js:\n",
    "                loss +=  self.Lambda*(self.b[i]-self.b[j])**2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G.p          Y.p          test_mask.p  y_test.p\r\n",
      "X.p          Y_true.p     train_mask.p y_train.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls graph1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes 18416\n",
      "number of edges 302406\n",
      "(18416, 117)\n"
     ]
    }
   ],
   "source": [
    "G3 = pickle.load(open(\"graph1/G.p\"))\n",
    "for u,v in G3.edges():\n",
    "    G3[u][v]['pos_edge_prob'] = 1\n",
    "for i in range(G3.number_of_nodes()):\n",
    "    G3.node[i]['pos_node_prob'] = 1\n",
    "# get all the nodes of the graph\n",
    "nodes = G3.nodes()\n",
    "# get some statistics about the graph\n",
    "print('number of nodes',G3.number_of_nodes())\n",
    "print('number of edges',G3.number_of_edges())\n",
    "\n",
    "y_train = pickle.load( open( \"graph1/y_train.p\", \"rb\" ) )\n",
    "y_true = pickle.load( open( \"graph1/Y_true.p\", \"rb\" ) )\n",
    "y_test = pickle.load( open( \"graph1/y_test.p\", \"rb\" ) )\n",
    "train_mask = pickle.load( open( \"graph1/train_mask.p\", \"rb\" ) )\n",
    "test_mask = pickle.load( open( \"graph1/test_mask.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "Y_train = np.zeros(G3.number_of_nodes())\n",
    "for i in range(len(Y_train)):\n",
    "    if y_train[i,0]==1:\n",
    "        Y_train[i] = -1\n",
    "    if y_train[i,1] ==1:\n",
    "        Y_train[i]=1\n",
    "Y_true = np.zeros(G3.number_of_nodes())\n",
    "for i in range(len(Y_true)):\n",
    "    if y_true[i,0]==1:\n",
    "        Y_true[i] = -1\n",
    "    if y_true[i,1] ==1:\n",
    "        Y_true[i]=1\n",
    "\n",
    "# Load feature matrix, select two features for the ADMM training\n",
    "X = pickle.load( open( \"graph1/X.p\", \"rb\" ) )\n",
    "print(X.shape)\n",
    "X = X[:,[2,116]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes 18416\n",
      "number of edges 302406\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "expcntDict = collections.defaultdict(dict)\n",
    "expamountDict = collections.defaultdict(dict)\n",
    "\n",
    "comset = set()\n",
    "nodes = set()\n",
    "edgecnt = 0;\n",
    "\n",
    "for edge in G3.edges():\n",
    "    src = edge[0]\n",
    "    target = edge[1]\n",
    "    edgecnt = edgecnt+1\n",
    "    nodes.add(src)\n",
    "    nodes.add(target)\n",
    "    expamountDict[src][target]=1\n",
    "    expamountDict[target][src]=1\n",
    "    expcntDict[src][target]=1\n",
    "    expcntDict[target][src]=1\n",
    "    nodecnt = len(nodes)\n",
    "# nodes is the set of nodes\n",
    "# nodecnt is the count of nodes in the graph\n",
    "# egdecnt is the count of edges in the graph\n",
    "print('number of nodes', nodecnt)\n",
    "print('number of edges', edgecnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lambda = 0.1\n",
    "Rho = 1.0\n",
    "Threshold = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X[train_mask], Y_train[train_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 0\n",
      "objective = 11858.0556526\n",
      "loss is 12802.0045168, old loss is 11858.0556526 at iteration 0.0\n",
      "loss is 11532.2901744, old loss is 11858.0556526 at iteration 0.1\n",
      "loss is 11368.1903594, old loss is 11858.0556526 at iteration 0.2\n",
      "loss is 11279.8691429, old loss is 11858.0556526 at iteration 0.3\n",
      "loss is 11221.4558562, old loss is 11858.0556526 at iteration 0.4\n",
      "loss is 11178.4834231, old loss is 11858.0556526 at iteration 0.5\n",
      "update W iteration 0.0\n",
      "anticipate the loss to decrease from 11178.4834231 by 0.665385519499\n",
      "learning rate: 1e-05\n",
      "max in gradient is :364.795599228\n",
      "oldloss:11178.4834231,newloss:11177.1672397\n",
      "update W iteration 0.1\n",
      "anticipate the loss to decrease from 11177.1672397 by 0.636535391633\n",
      "learning rate: 1e-05\n",
      "max in gradient is :356.799376159\n",
      "oldloss:11177.1672397,newloss:11175.9081084\n",
      "finished w 2.7459359169 seconds at iteration 0\n",
      "loss is 11175.9081084, old loss is 11858.0556526 at iteration 0\n",
      "loss is 11141.5003679, old loss is 11175.9081084 at iteration 1.0\n",
      "loss is 11112.2708371, old loss is 11175.9081084 at iteration 1.1\n",
      "loss is 11086.238237, old loss is 11175.9081084 at iteration 1.2\n",
      "loss is 11062.340133, old loss is 11175.9081084 at iteration 1.3\n",
      "loss is 11039.9798713, old loss is 11175.9081084 at iteration 1.4\n",
      "loss is 11018.812797, old loss is 11175.9081084 at iteration 1.5\n",
      "update W iteration 1.0\n",
      "anticipate the loss to decrease from 11018.812797 by 0.27319550787\n",
      "learning rate: 1e-05\n",
      "max in gradient is :233.747697654\n",
      "oldloss:11018.812797,newloss:11018.2721078\n",
      "update W iteration 1.1\n",
      "anticipate the loss to decrease from 11018.2721078 by 0.261912272249\n",
      "learning rate: 1e-05\n",
      "max in gradient is :228.869687793\n",
      "oldloss:11018.2721078,newloss:11017.7537455\n",
      "finished w 2.75534892082 seconds at iteration 1\n",
      "loss is 11017.7537455, old loss is 11175.9081084 at iteration 1\n",
      "loss is 10997.4076705, old loss is 11017.7537455 at iteration 2.0\n",
      "loss is 10978.0223749, old loss is 11017.7537455 at iteration 2.1\n",
      "loss is 10959.4468955, old loss is 11017.7537455 at iteration 2.2\n",
      "loss is 10941.6056633, old loss is 11017.7537455 at iteration 2.3\n",
      "loss is 10924.4444411, old loss is 11017.7537455 at iteration 2.4\n",
      "loss is 10907.9185793, old loss is 11017.7537455 at iteration 2.5\n",
      "update W iteration 2.0\n",
      "anticipate the loss to decrease from 10907.9185793 by 0.142322330898\n",
      "learning rate: 1e-05\n",
      "max in gradient is :168.711234465\n",
      "oldloss:10907.9185793,newloss:10907.6368204\n",
      "update W iteration 2.1\n",
      "anticipate the loss to decrease from 10907.6368204 by 0.136609752127\n",
      "learning rate: 1e-05\n",
      "max in gradient is :165.290536939\n",
      "oldloss:10907.6368204,newloss:10907.3663694\n",
      "finished w 3.78538918495 seconds at iteration 2\n",
      "loss is 10907.3663694, old loss is 11017.7537455 at iteration 2\n",
      "loss is 10891.3087116, old loss is 10907.3663694 at iteration 3.0\n",
      "loss is 10875.8870634, old loss is 10907.3663694 at iteration 3.1\n",
      "loss is 10861.027381, old loss is 10907.3663694 at iteration 3.2\n",
      "loss is 10846.6913644, old loss is 10907.3663694 at iteration 3.3\n",
      "loss is 10832.8478118, old loss is 10907.3663694 at iteration 3.4\n",
      "loss is 10819.4688644, old loss is 10907.3663694 at iteration 3.5\n",
      "update W iteration 3.0\n",
      "anticipate the loss to decrease from 10819.4688644 by 0.0793654842116\n",
      "learning rate: 1e-05\n",
      "max in gradient is :125.984978902\n",
      "oldloss:10819.4688644,newloss:10819.3117091\n",
      "update W iteration 3.1\n",
      "anticipate the loss to decrease from 10819.3117091 by 0.0762456387671\n",
      "learning rate: 1e-05\n",
      "max in gradient is :123.48377867\n",
      "oldloss:10819.3117091,newloss:10819.1607309\n",
      "finished w 2.86974310875 seconds at iteration 3\n",
      "loss is 10819.1607309, old loss is 10907.3663694 at iteration 3\n",
      "loss is 10806.1251031, old loss is 10819.1607309 at iteration 4.0\n",
      "loss is 10793.5540687, old loss is 10819.1607309 at iteration 4.1\n",
      "loss is 10781.3978756, old loss is 10819.1607309 at iteration 4.2\n",
      "loss is 10769.6288589, old loss is 10819.1607309 at iteration 4.3\n",
      "loss is 10758.2253234, old loss is 10819.1607309 at iteration 4.4\n",
      "loss is 10747.1672762, old loss is 10819.1607309 at iteration 4.5\n",
      "update W iteration 4.0\n",
      "anticipate the loss to decrease from 10747.1672762 by 0.0448946030071\n",
      "learning rate: 1e-05\n",
      "max in gradient is :94.7529476125\n",
      "oldloss:10747.1672762,newloss:10747.0783633\n",
      "finished w 1.94381189346 seconds at iteration 4\n",
      "loss is 10747.0783633, old loss is 10819.1607309 at iteration 4\n",
      "loss is 10736.313141, old loss is 10747.0783633 at iteration 5.0\n",
      "loss is 10725.876764, old loss is 10747.0783633 at iteration 5.1\n",
      "loss is 10715.7441816, old loss is 10747.0783633 at iteration 5.2\n",
      "loss is 10705.8992927, old loss is 10747.0783633 at iteration 5.3\n",
      "loss is 10696.3281941, old loss is 10747.0783633 at iteration 5.4\n",
      "loss is 10687.0181708, old loss is 10747.0783633 at iteration 5.5\n",
      "update W iteration 5.0\n",
      "anticipate the loss to decrease from 10687.0181708 by 0.0261673055396\n",
      "learning rate: 1e-05\n",
      "max in gradient is :72.3376017359\n",
      "oldloss:10687.0181708,newloss:10686.9663398\n",
      "finished w 1.63451290131 seconds at iteration 5\n",
      "loss is 10686.9663398, old loss is 10747.0783633 at iteration 5\n",
      "loss is 10677.8794448, old loss is 10686.9663398 at iteration 6.0\n",
      "loss is 10669.0429218, old loss is 10686.9663398 at iteration 6.1\n",
      "loss is 10660.4400371, old loss is 10686.9663398 at iteration 6.2\n",
      "loss is 10652.0588353, old loss is 10686.9663398 at iteration 6.3\n",
      "loss is 10643.8900812, old loss is 10686.9663398 at iteration 6.4\n",
      "loss is 10635.9248649, old loss is 10686.9663398 at iteration 6.5\n",
      "update W iteration 6.0\n",
      "anticipate the loss to decrease from 10635.9248649 by 0.0148362696343\n",
      "learning rate: 1e-05\n",
      "max in gradient is :54.4662732415\n",
      "oldloss:10635.9248649,newloss:10635.8954745\n",
      "finished w 1.6919028759 seconds at iteration 6\n",
      "loss is 10635.8954745, old loss is 10686.9663398 at iteration 6\n",
      "loss is 10628.1062934, old loss is 10635.8954745 at iteration 7.0\n",
      "loss is 10620.5129424, old loss is 10635.8954745 at iteration 7.1\n",
      "loss is 10613.1034365, old loss is 10635.8954745 at iteration 7.2\n",
      "loss is 10605.8698623, old loss is 10635.8954745 at iteration 7.3\n",
      "loss is 10598.8050028, old loss is 10635.8954745 at iteration 7.4\n",
      "loss is 10591.9020057, old loss is 10635.8954745 at iteration 7.5\n",
      "update W iteration 7.0\n",
      "anticipate the loss to decrease from 10591.9020057 by 0.00799501400247\n",
      "learning rate: 1e-05\n",
      "max in gradient is :39.9795793469\n",
      "oldloss:10591.9020057,newloss:10591.8861661\n",
      "finished w 2.05654907227 seconds at iteration 7\n",
      "loss is 10591.8861661, old loss is 10635.8954745 at iteration 7\n",
      "loss is 10585.1257935, old loss is 10591.8861661 at iteration 8.0\n",
      "loss is 10578.521766, old loss is 10591.8861661 at iteration 8.1\n",
      "loss is 10572.0657268, old loss is 10591.8861661 at iteration 8.2\n",
      "loss is 10565.751298, old loss is 10591.8861661 at iteration 8.3\n",
      "loss is 10559.573104, old loss is 10591.8861661 at iteration 8.4\n",
      "loss is 10553.5269442, old loss is 10591.8861661 at iteration 8.5\n",
      "update W iteration 8.0\n",
      "anticipate the loss to decrease from 10553.5269442 by 0.00395001470457\n",
      "learning rate: 1e-05\n",
      "max in gradient is :28.0962929598\n",
      "oldloss:10553.5269442,newloss:10553.5191177\n",
      "finished w 2.05488395691 seconds at iteration 8\n",
      "loss is 10553.5191177, old loss is 10591.8861661 at iteration 8\n",
      "loss is 10547.590348, old loss is 10553.5191177 at iteration 9.0\n",
      "loss is 10541.7886518, old loss is 10553.5191177 at iteration 9.1\n",
      "loss is 10536.1070017, old loss is 10553.5191177 at iteration 9.2\n",
      "loss is 10530.5418765, old loss is 10553.5191177 at iteration 9.3\n",
      "loss is 10525.0889326, old loss is 10553.5191177 at iteration 9.4\n",
      "loss is 10519.7442623, old loss is 10553.5191177 at iteration 9.5\n",
      "update W iteration 9.0\n",
      "anticipate the loss to decrease from 10519.7442623 by 0.00167122129197\n",
      "learning rate: 1e-05\n",
      "max in gradient is :18.2666725495\n",
      "oldloss:10519.7442623,newloss:10519.7409506\n",
      "finished w 1.62891387939 seconds at iteration 9\n",
      "loss is 10519.7409506, old loss is 10553.5191177 at iteration 9\n",
      "loss is 10514.4951708, old loss is 10519.7409506 at iteration 10.0\n",
      "loss is 10509.3534071, old loss is 10519.7409506 at iteration 10.1\n",
      "loss is 10504.3109553, old loss is 10519.7409506 at iteration 10.2\n",
      "loss is 10499.3647719, old loss is 10519.7409506 at iteration 10.3\n",
      "loss is 10494.5113843, old loss is 10519.7409506 at iteration 10.4\n",
      "loss is 10489.748707, old loss is 10519.7409506 at iteration 10.5\n",
      "update W iteration 10.0\n",
      "anticipate the loss to decrease from 10489.748707 by 0.000511774898648\n",
      "learning rate: 1e-05\n",
      "max in gradient is :10.0898601024\n",
      "oldloss:10489.748707,newloss:10489.7476928\n",
      "finished w 2.26862883568 seconds at iteration 10\n",
      "loss is 10489.7476928, old loss is 10519.7409506 at iteration 10\n",
      "loss is 10485.0697201, old loss is 10489.7476928 at iteration 11.0\n",
      "update W iteration 11.0\n",
      "anticipate the loss to decrease from 10485.0697201 by 0.000383272633995\n",
      "learning rate: 1e-05\n",
      "max in gradient is :8.72400068088\n",
      "oldloss:10485.0697201,newloss:10485.0689606\n",
      "finished w 1.7627658844 seconds at iteration 11\n",
      "loss is 10485.0689606, old loss is 10489.7476928 at iteration 11\n",
      "total iterations = 11\n",
      "total time = 381.951632977\n",
      "382.743108988\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "C = ADMM(X, Y_train, expamountDict, nodes, edgecnt,\n",
    "         Lambda, Rho, train_mask,test_mask,Y_true, Threshold, logistic.coef_, logistic.intercept_)\n",
    "start = time.time()\n",
    "C.runADMM_Grid()\n",
    "end = time.time()\n",
    "print(end- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.67060179, -0.03397293]),\n",
       " array([ 0.09799436,  0.39766001,  1.18838771, ..., -0.79955114,\n",
       "         0.30934177,  0.4990298 ]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.W, C.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAGHCAYAAAD89VV0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x135229d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Training loss per iteration',fontsize=30)\n",
    "plt.plot(range(len(C.losses)),C.losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
